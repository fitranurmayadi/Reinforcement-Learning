{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\robot_sim\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run TRPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 328       |\n",
      "|    ep_rew_mean     | -9.47e+04 |\n",
      "| time/              |           |\n",
      "|    fps             | 241       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 8         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 338       |\n",
      "|    ep_rew_mean            | -9.08e+04 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 211       |\n",
      "|    iterations             | 2         |\n",
      "|    time_elapsed           | 19        |\n",
      "|    total_timesteps        | 4096      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -0.000102 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00412   |\n",
      "|    learning_rate          | 0.0001    |\n",
      "|    n_updates              | 1         |\n",
      "|    policy_objective       | 0.136     |\n",
      "|    std                    | 1.01      |\n",
      "|    value_loss             | 2.2e+07   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 288       |\n",
      "|    ep_rew_mean            | -8.07e+04 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 192       |\n",
      "|    iterations             | 3         |\n",
      "|    time_elapsed           | 31        |\n",
      "|    total_timesteps        | 6144      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | 0.000456  |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00492   |\n",
      "|    learning_rate          | 0.0001    |\n",
      "|    n_updates              | 2         |\n",
      "|    policy_objective       | 0.11      |\n",
      "|    std                    | 1.01      |\n",
      "|    value_loss             | 1.65e+07  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 279       |\n",
      "|    ep_rew_mean            | -8.04e+04 |\n",
      "| time/                     |           |\n",
      "|    fps                    | 187       |\n",
      "|    iterations             | 4         |\n",
      "|    time_elapsed           | 43        |\n",
      "|    total_timesteps        | 8192      |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -6.56e-06 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00415   |\n",
      "|    learning_rate          | 0.0001    |\n",
      "|    n_updates              | 3         |\n",
      "|    policy_objective       | 0.134     |\n",
      "|    std                    | 1.01      |\n",
      "|    value_loss             | 2.44e+07  |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\robot_sim\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch as th\n",
    "from LunarLander3DEnv.envs import LunarLander3DEnv  # Pastikan environment sudah terdaftar (register)\n",
    "# Import algoritma dari stable-baselines3\n",
    "# Tambahkan library sb3-contrib jika menggunakan TRPO\n",
    "from stable_baselines3 import PPO, A2C, DDPG, SAC, TD3\n",
    "from sb3_contrib import TRPO\n",
    "\n",
    "# TRPO tersedia dari sb3-contrib\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "\n",
    "# Pilih algoritma yang ingin digunakan: \"PPO\", \"A2C\", \"DDPG\", \"SAC\", \"TD3\", atau \"TRPO\"\n",
    "algorithm = \"TRPO\"\n",
    "\n",
    "# Buat environment\n",
    "env = gym.make(\"LunarLander3DEnv-v0\", render_mode=None)\n",
    "\n",
    "# Inisialisasi model sesuai algoritma yang dipilih\n",
    "model = None\n",
    "if algorithm == \"PPO\":\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=3e-4,  # Bisa diturunkan jika training terlalu lambat\n",
    "        n_steps=2048,  # Lebih besar agar bisa menangkap lebih banyak trajektori\n",
    "        batch_size=64,  # Cukup kecil untuk menjaga stabilitas\n",
    "        n_epochs=10,  # Jumlah update per batch\n",
    "        gamma=0.99,  # Discount factor untuk jangka panjang\n",
    "        gae_lambda=0.95,  # Generalized Advantage Estimation\n",
    "        clip_range=0.2,  # PPO Clipping\n",
    "        ent_coef=0.01,  # Menambah eksplorasi dengan entropy loss\n",
    "        vf_coef=0.5,  # Koefisien untuk fungsi nilai (value function)\n",
    "        policy_kwargs=dict(net_arch=dict(pi=[256, 256], vf=[256, 256])),\n",
    "        verbose=1,\n",
    "        device=\"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "elif algorithm == \"SAC\":\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=3e-4,\n",
    "        buffer_size=500_000,  # Replay buffer lebih besar untuk kontrol kontinu\n",
    "        batch_size=128,  # SAC lebih stabil dengan batch besar\n",
    "        tau=0.005,  # Soft update factor\n",
    "        gamma=0.99,\n",
    "        ent_coef=\"auto_0.1\",  # Entropy coefficient adaptif\n",
    "        use_sde=True,  # State Dependent Exploration (untuk eksplorasi lebih baik)\n",
    "        policy_kwargs=dict(net_arch=dict(pi=[256, 256], qf=[256, 256])),\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "elif algorithm == \"TD3\":\n",
    "    model = TD3(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=1e-3,  # TD3 lebih cepat belajar dengan LR lebih tinggi\n",
    "        buffer_size=500_000,\n",
    "        batch_size=128,\n",
    "        tau=0.005,\n",
    "        gamma=0.99,\n",
    "        policy_delay=2,  # Delay untuk update policy lebih stabil\n",
    "        target_policy_noise=0.2,  # Tambahkan noise agar eksplorasi lebih baik\n",
    "        target_noise_clip=0.5,\n",
    "        policy_kwargs=dict(net_arch=[256, 256]),\n",
    "        verbose=1,\n",
    "    )\n",
    "elif algorithm == \"TRPO\":\n",
    "    model = TRPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=1e-4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        #max_kl=0.01,  # Batas maksimum perubahan policy\n",
    "        #ent_coef=0.01,\n",
    "        #vf_coef=0.5,\n",
    "        policy_kwargs=dict(net_arch=[256, 256]),\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Algoritma yang dipilih tidak didukung. Pilih salah satu: PPO, SAC, TD3, TRPO.\")\n",
    "\n",
    "# Setup callbacks untuk menyimpan model dan evaluasi secara berkala\n",
    "total_timesteps_learning = 100_000\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=int(total_timesteps_learning / 10),\n",
    "    save_path='./models/',\n",
    "    name_prefix=f'{algorithm}_checkpoint_v1_'\n",
    ")\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    best_model_save_path=\"./logs/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=total_timesteps_learning / 10,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "# Mulai proses pelatihan\n",
    "model.learn(total_timesteps=total_timesteps_learning, callback=[checkpoint_callback, eval_callback])\n",
    "\n",
    "# Simpan model yang sudah dilatih\n",
    "model.save(f\"ll3d_{algorithm}\")\n",
    "print(\"Training is finished\")\n",
    "\n",
    "# Evaluasi agent\n",
    "n_eval_episodes = 10\n",
    "render = False  # Ubah ke True jika ingin melihat visualisasi evaluasi\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=n_eval_episodes, render=render)\n",
    "\n",
    "print(f\"Mean Reward over {n_eval_episodes} episodes: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "env.close()\n",
    "del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\robot_sim\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run TRPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -67353.34\n",
      "Episode 2: Total Reward = -4242.19\n",
      "Episode 3: Total Reward = -7493.94\n",
      "Episode 4: Total Reward = -8167.36\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "from LunarLander3DEnv.envs import LunarLander3DEnv\n",
    "# Pilih algoritma yang digunakan saat training, misal \"PPO\"\n",
    "\n",
    "algorithm = \"TRPO\"\n",
    "model_path = f\"ll3d_{algorithm}\"  # Nama file model yang telah disimpan\n",
    "\n",
    "# Buat environment dengan render_mode \"human\" agar dapat melihat visualisasi\n",
    "env = gym.make(\"LunarLander3DEnv-v0\", action_type=\"discrete\", render_mode=\"human\")\n",
    "\n",
    "# Muat model berdasarkan algoritma yang dipilih\n",
    "if algorithm == \"PPO\":\n",
    "    from stable_baselines3 import PPO\n",
    "    model = PPO.load(model_path, env=env)\n",
    "elif algorithm == \"A2C\":\n",
    "    from stable_baselines3 import A2C\n",
    "    model = A2C.load(model_path, env=env)\n",
    "elif algorithm == \"DDPG\":\n",
    "    from stable_baselines3 import DDPG\n",
    "    model = DDPG.load(model_path, env=env)\n",
    "elif algorithm == \"SAC\":\n",
    "    from stable_baselines3 import SAC\n",
    "    model = SAC.load(model_path, env=env)\n",
    "elif algorithm == \"TD3\":\n",
    "    from stable_baselines3 import TD3\n",
    "    model = TD3.load(model_path, env=env)\n",
    "elif algorithm == \"TRPO\":\n",
    "    from sb3_contrib import TRPO\n",
    "    model = TRPO.load(model_path, env=env)\n",
    "else:\n",
    "    raise ValueError(\"Algoritma yang dipilih tidak didukung.\")\n",
    "\n",
    "# Jalankan beberapa episode untuk inferensi\n",
    "num_episodes = 10\n",
    "for episode in range(1, num_episodes+1):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    while not done:\n",
    "        # Prediksi aksi dengan deterministik\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        env.render()  # Pastikan render dipanggil agar kamera dan visualisasi diupdate\n",
    "        #time.sleep(1/60)  # Optional: delay untuk melambatkan tampilan\n",
    "        done = terminated or truncated\n",
    "    print(f\"Episode {episode}: Total Reward = {total_reward:.2f}\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robot_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
